{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hitters data set homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters = pd.read_csv('Hitters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = hitters.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>League</th>\n",
       "      <th>Division</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "      <th>NewLeague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>446</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>69</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>N</td>\n",
       "      <td>W</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>475.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>63</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>480.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>225</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>91.5</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n",
       "0    293    66      1    30   29     14      1     293     66       1     30   \n",
       "1    315    81      7    24   38     39     14    3449    835      69    321   \n",
       "2    479   130     18    66   72     76      3    1624    457      63    224   \n",
       "3    496   141     20    65   78     37     11    5628   1575     225    828   \n",
       "4    321    87     10    39   42     30      2     396    101      12     48   \n",
       "\n",
       "   CRBI  CWalks League Division  PutOuts  Assists  Errors  Salary NewLeague  \n",
       "0    29      14      A        E      446       33      20     NaN         A  \n",
       "1   414     375      N        W      632       43      10   475.0         N  \n",
       "2   266     263      A        W      880       82      14   480.0         A  \n",
       "3   838     354      N        E      200       11       3   500.0         N  \n",
       "4    46      33      N        E      805       40       4    91.5         N  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "      <th>League_N</th>\n",
       "      <th>Division_W</th>\n",
       "      <th>NewLeague_N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>446</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>69</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>475.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>63</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>480.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>225</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>91.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n",
       "0    293    66      1    30   29     14      1     293     66       1     30   \n",
       "1    315    81      7    24   38     39     14    3449    835      69    321   \n",
       "2    479   130     18    66   72     76      3    1624    457      63    224   \n",
       "3    496   141     20    65   78     37     11    5628   1575     225    828   \n",
       "4    321    87     10    39   42     30      2     396    101      12     48   \n",
       "\n",
       "   CRBI  CWalks  PutOuts  Assists  Errors  Salary  League_N  Division_W  \\\n",
       "0    29      14      446       33      20     NaN         0           0   \n",
       "1   414     375      632       43      10   475.0         1           1   \n",
       "2   266     263      880       82      14   480.0         0           1   \n",
       "3   838     354      200       11       3   500.0         1           0   \n",
       "4    46      33      805       40       4    91.5         1           0   \n",
       "\n",
       "   NewLeague_N  \n",
       "0            0  \n",
       "1            1  \n",
       "2            0  \n",
       "3            1  \n",
       "4            1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.get_dummies(df, columns = ['League', 'Division', 'NewLeague'], drop_first = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "      <th>League_N</th>\n",
       "      <th>Division_W</th>\n",
       "      <th>NewLeague_N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AtBat</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967939</td>\n",
       "      <td>0.592198</td>\n",
       "      <td>0.913060</td>\n",
       "      <td>0.820539</td>\n",
       "      <td>0.669845</td>\n",
       "      <td>0.047372</td>\n",
       "      <td>0.235526</td>\n",
       "      <td>0.252717</td>\n",
       "      <td>0.236659</td>\n",
       "      <td>0.266534</td>\n",
       "      <td>0.244053</td>\n",
       "      <td>0.166123</td>\n",
       "      <td>0.317550</td>\n",
       "      <td>0.353824</td>\n",
       "      <td>0.352117</td>\n",
       "      <td>0.394771</td>\n",
       "      <td>-0.101663</td>\n",
       "      <td>-0.045441</td>\n",
       "      <td>-0.057655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hits</th>\n",
       "      <td>0.967939</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.562158</td>\n",
       "      <td>0.922187</td>\n",
       "      <td>0.811073</td>\n",
       "      <td>0.641211</td>\n",
       "      <td>0.044767</td>\n",
       "      <td>0.227565</td>\n",
       "      <td>0.255815</td>\n",
       "      <td>0.202712</td>\n",
       "      <td>0.261787</td>\n",
       "      <td>0.232005</td>\n",
       "      <td>0.151818</td>\n",
       "      <td>0.310673</td>\n",
       "      <td>0.320455</td>\n",
       "      <td>0.310038</td>\n",
       "      <td>0.438675</td>\n",
       "      <td>-0.101038</td>\n",
       "      <td>-0.071143</td>\n",
       "      <td>-0.060608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HmRun</th>\n",
       "      <td>0.592198</td>\n",
       "      <td>0.562158</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.650988</td>\n",
       "      <td>0.855122</td>\n",
       "      <td>0.481014</td>\n",
       "      <td>0.116318</td>\n",
       "      <td>0.221882</td>\n",
       "      <td>0.220627</td>\n",
       "      <td>0.493227</td>\n",
       "      <td>0.262361</td>\n",
       "      <td>0.351979</td>\n",
       "      <td>0.233154</td>\n",
       "      <td>0.282923</td>\n",
       "      <td>-0.106329</td>\n",
       "      <td>0.039318</td>\n",
       "      <td>0.343028</td>\n",
       "      <td>-0.177258</td>\n",
       "      <td>-0.017206</td>\n",
       "      <td>-0.166044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Runs</th>\n",
       "      <td>0.913060</td>\n",
       "      <td>0.922187</td>\n",
       "      <td>0.650988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.798206</td>\n",
       "      <td>0.732213</td>\n",
       "      <td>0.004541</td>\n",
       "      <td>0.186497</td>\n",
       "      <td>0.204830</td>\n",
       "      <td>0.227913</td>\n",
       "      <td>0.250556</td>\n",
       "      <td>0.205976</td>\n",
       "      <td>0.182168</td>\n",
       "      <td>0.279347</td>\n",
       "      <td>0.220567</td>\n",
       "      <td>0.240475</td>\n",
       "      <td>0.419859</td>\n",
       "      <td>-0.150392</td>\n",
       "      <td>-0.076311</td>\n",
       "      <td>-0.115446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBI</th>\n",
       "      <td>0.820539</td>\n",
       "      <td>0.811073</td>\n",
       "      <td>0.855122</td>\n",
       "      <td>0.798206</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615997</td>\n",
       "      <td>0.146168</td>\n",
       "      <td>0.294688</td>\n",
       "      <td>0.308201</td>\n",
       "      <td>0.441771</td>\n",
       "      <td>0.323285</td>\n",
       "      <td>0.393184</td>\n",
       "      <td>0.250914</td>\n",
       "      <td>0.343186</td>\n",
       "      <td>0.106591</td>\n",
       "      <td>0.193370</td>\n",
       "      <td>0.449457</td>\n",
       "      <td>-0.145597</td>\n",
       "      <td>-0.075531</td>\n",
       "      <td>-0.120124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Walks</th>\n",
       "      <td>0.669845</td>\n",
       "      <td>0.641211</td>\n",
       "      <td>0.481014</td>\n",
       "      <td>0.732213</td>\n",
       "      <td>0.615997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.136475</td>\n",
       "      <td>0.277175</td>\n",
       "      <td>0.280671</td>\n",
       "      <td>0.332473</td>\n",
       "      <td>0.338478</td>\n",
       "      <td>0.308631</td>\n",
       "      <td>0.424507</td>\n",
       "      <td>0.299515</td>\n",
       "      <td>0.149656</td>\n",
       "      <td>0.129382</td>\n",
       "      <td>0.443867</td>\n",
       "      <td>-0.045337</td>\n",
       "      <td>-0.059086</td>\n",
       "      <td>-0.020029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Years</th>\n",
       "      <td>0.047372</td>\n",
       "      <td>0.044767</td>\n",
       "      <td>0.116318</td>\n",
       "      <td>0.004541</td>\n",
       "      <td>0.146168</td>\n",
       "      <td>0.136475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920289</td>\n",
       "      <td>0.903631</td>\n",
       "      <td>0.726872</td>\n",
       "      <td>0.882877</td>\n",
       "      <td>0.868812</td>\n",
       "      <td>0.838533</td>\n",
       "      <td>-0.004684</td>\n",
       "      <td>-0.080638</td>\n",
       "      <td>-0.162140</td>\n",
       "      <td>0.400657</td>\n",
       "      <td>-0.047261</td>\n",
       "      <td>0.021126</td>\n",
       "      <td>-0.042919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAtBat</th>\n",
       "      <td>0.235526</td>\n",
       "      <td>0.227565</td>\n",
       "      <td>0.221882</td>\n",
       "      <td>0.186497</td>\n",
       "      <td>0.294688</td>\n",
       "      <td>0.277175</td>\n",
       "      <td>0.920289</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995063</td>\n",
       "      <td>0.798836</td>\n",
       "      <td>0.983345</td>\n",
       "      <td>0.949219</td>\n",
       "      <td>0.906501</td>\n",
       "      <td>0.062283</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>-0.066922</td>\n",
       "      <td>0.526135</td>\n",
       "      <td>-0.028738</td>\n",
       "      <td>0.022965</td>\n",
       "      <td>-0.016502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHits</th>\n",
       "      <td>0.252717</td>\n",
       "      <td>0.255815</td>\n",
       "      <td>0.220627</td>\n",
       "      <td>0.204830</td>\n",
       "      <td>0.308201</td>\n",
       "      <td>0.280671</td>\n",
       "      <td>0.903631</td>\n",
       "      <td>0.995063</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.783306</td>\n",
       "      <td>0.984609</td>\n",
       "      <td>0.945141</td>\n",
       "      <td>0.890954</td>\n",
       "      <td>0.076547</td>\n",
       "      <td>-0.002523</td>\n",
       "      <td>-0.062756</td>\n",
       "      <td>0.548910</td>\n",
       "      <td>-0.025790</td>\n",
       "      <td>0.013584</td>\n",
       "      <td>-0.010047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHmRun</th>\n",
       "      <td>0.236659</td>\n",
       "      <td>0.202712</td>\n",
       "      <td>0.493227</td>\n",
       "      <td>0.227913</td>\n",
       "      <td>0.441771</td>\n",
       "      <td>0.332473</td>\n",
       "      <td>0.726872</td>\n",
       "      <td>0.798836</td>\n",
       "      <td>0.783306</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.820243</td>\n",
       "      <td>0.929484</td>\n",
       "      <td>0.799983</td>\n",
       "      <td>0.112724</td>\n",
       "      <td>-0.158511</td>\n",
       "      <td>-0.138115</td>\n",
       "      <td>0.524931</td>\n",
       "      <td>-0.098383</td>\n",
       "      <td>0.006783</td>\n",
       "      <td>-0.097461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRuns</th>\n",
       "      <td>0.266534</td>\n",
       "      <td>0.261787</td>\n",
       "      <td>0.262361</td>\n",
       "      <td>0.250556</td>\n",
       "      <td>0.323285</td>\n",
       "      <td>0.338478</td>\n",
       "      <td>0.882877</td>\n",
       "      <td>0.983345</td>\n",
       "      <td>0.984609</td>\n",
       "      <td>0.820243</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.943769</td>\n",
       "      <td>0.927807</td>\n",
       "      <td>0.064180</td>\n",
       "      <td>-0.022978</td>\n",
       "      <td>-0.084395</td>\n",
       "      <td>0.562678</td>\n",
       "      <td>-0.054666</td>\n",
       "      <td>-0.000935</td>\n",
       "      <td>-0.042882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRBI</th>\n",
       "      <td>0.244053</td>\n",
       "      <td>0.232005</td>\n",
       "      <td>0.351979</td>\n",
       "      <td>0.205976</td>\n",
       "      <td>0.393184</td>\n",
       "      <td>0.308631</td>\n",
       "      <td>0.868812</td>\n",
       "      <td>0.949219</td>\n",
       "      <td>0.945141</td>\n",
       "      <td>0.929484</td>\n",
       "      <td>0.943769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884726</td>\n",
       "      <td>0.110098</td>\n",
       "      <td>-0.079387</td>\n",
       "      <td>-0.100990</td>\n",
       "      <td>0.566966</td>\n",
       "      <td>-0.052684</td>\n",
       "      <td>0.013213</td>\n",
       "      <td>-0.045737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWalks</th>\n",
       "      <td>0.166123</td>\n",
       "      <td>0.151818</td>\n",
       "      <td>0.233154</td>\n",
       "      <td>0.182168</td>\n",
       "      <td>0.250914</td>\n",
       "      <td>0.424507</td>\n",
       "      <td>0.838533</td>\n",
       "      <td>0.906501</td>\n",
       "      <td>0.890954</td>\n",
       "      <td>0.799983</td>\n",
       "      <td>0.927807</td>\n",
       "      <td>0.884726</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058638</td>\n",
       "      <td>-0.039130</td>\n",
       "      <td>-0.118475</td>\n",
       "      <td>0.489822</td>\n",
       "      <td>-0.050884</td>\n",
       "      <td>-0.005183</td>\n",
       "      <td>-0.051726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PutOuts</th>\n",
       "      <td>0.317550</td>\n",
       "      <td>0.310673</td>\n",
       "      <td>0.282923</td>\n",
       "      <td>0.279347</td>\n",
       "      <td>0.343186</td>\n",
       "      <td>0.299515</td>\n",
       "      <td>-0.004684</td>\n",
       "      <td>0.062283</td>\n",
       "      <td>0.076547</td>\n",
       "      <td>0.112724</td>\n",
       "      <td>0.064180</td>\n",
       "      <td>0.110098</td>\n",
       "      <td>0.058638</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025024</td>\n",
       "      <td>0.109972</td>\n",
       "      <td>0.300480</td>\n",
       "      <td>0.023984</td>\n",
       "      <td>-0.005647</td>\n",
       "      <td>0.032833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Assists</th>\n",
       "      <td>0.353824</td>\n",
       "      <td>0.320455</td>\n",
       "      <td>-0.106329</td>\n",
       "      <td>0.220567</td>\n",
       "      <td>0.106591</td>\n",
       "      <td>0.149656</td>\n",
       "      <td>-0.080638</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>-0.002523</td>\n",
       "      <td>-0.158511</td>\n",
       "      <td>-0.022978</td>\n",
       "      <td>-0.079387</td>\n",
       "      <td>-0.039130</td>\n",
       "      <td>-0.025024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.706362</td>\n",
       "      <td>0.025436</td>\n",
       "      <td>0.045527</td>\n",
       "      <td>-0.013852</td>\n",
       "      <td>0.039066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Errors</th>\n",
       "      <td>0.352117</td>\n",
       "      <td>0.310038</td>\n",
       "      <td>0.039318</td>\n",
       "      <td>0.240475</td>\n",
       "      <td>0.193370</td>\n",
       "      <td>0.129382</td>\n",
       "      <td>-0.162140</td>\n",
       "      <td>-0.066922</td>\n",
       "      <td>-0.062756</td>\n",
       "      <td>-0.138115</td>\n",
       "      <td>-0.084395</td>\n",
       "      <td>-0.100990</td>\n",
       "      <td>-0.118475</td>\n",
       "      <td>0.109972</td>\n",
       "      <td>0.706362</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005401</td>\n",
       "      <td>0.078508</td>\n",
       "      <td>-0.023121</td>\n",
       "      <td>0.050140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Salary</th>\n",
       "      <td>0.394771</td>\n",
       "      <td>0.438675</td>\n",
       "      <td>0.343028</td>\n",
       "      <td>0.419859</td>\n",
       "      <td>0.449457</td>\n",
       "      <td>0.443867</td>\n",
       "      <td>0.400657</td>\n",
       "      <td>0.526135</td>\n",
       "      <td>0.548910</td>\n",
       "      <td>0.524931</td>\n",
       "      <td>0.562678</td>\n",
       "      <td>0.566966</td>\n",
       "      <td>0.489822</td>\n",
       "      <td>0.300480</td>\n",
       "      <td>0.025436</td>\n",
       "      <td>-0.005401</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.014282</td>\n",
       "      <td>-0.192514</td>\n",
       "      <td>-0.002834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>League_N</th>\n",
       "      <td>-0.101663</td>\n",
       "      <td>-0.101038</td>\n",
       "      <td>-0.177258</td>\n",
       "      <td>-0.150392</td>\n",
       "      <td>-0.145597</td>\n",
       "      <td>-0.045337</td>\n",
       "      <td>-0.047261</td>\n",
       "      <td>-0.028738</td>\n",
       "      <td>-0.025790</td>\n",
       "      <td>-0.098383</td>\n",
       "      <td>-0.054666</td>\n",
       "      <td>-0.052684</td>\n",
       "      <td>-0.050884</td>\n",
       "      <td>0.023984</td>\n",
       "      <td>0.045527</td>\n",
       "      <td>0.078508</td>\n",
       "      <td>-0.014282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004067</td>\n",
       "      <td>0.881041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Division_W</th>\n",
       "      <td>-0.045441</td>\n",
       "      <td>-0.071143</td>\n",
       "      <td>-0.017206</td>\n",
       "      <td>-0.076311</td>\n",
       "      <td>-0.075531</td>\n",
       "      <td>-0.059086</td>\n",
       "      <td>0.021126</td>\n",
       "      <td>0.022965</td>\n",
       "      <td>0.013584</td>\n",
       "      <td>0.006783</td>\n",
       "      <td>-0.000935</td>\n",
       "      <td>0.013213</td>\n",
       "      <td>-0.005183</td>\n",
       "      <td>-0.005647</td>\n",
       "      <td>-0.013852</td>\n",
       "      <td>-0.023121</td>\n",
       "      <td>-0.192514</td>\n",
       "      <td>-0.004067</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.010155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NewLeague_N</th>\n",
       "      <td>-0.057655</td>\n",
       "      <td>-0.060608</td>\n",
       "      <td>-0.166044</td>\n",
       "      <td>-0.115446</td>\n",
       "      <td>-0.120124</td>\n",
       "      <td>-0.020029</td>\n",
       "      <td>-0.042919</td>\n",
       "      <td>-0.016502</td>\n",
       "      <td>-0.010047</td>\n",
       "      <td>-0.097461</td>\n",
       "      <td>-0.042882</td>\n",
       "      <td>-0.045737</td>\n",
       "      <td>-0.051726</td>\n",
       "      <td>0.032833</td>\n",
       "      <td>0.039066</td>\n",
       "      <td>0.050140</td>\n",
       "      <td>-0.002834</td>\n",
       "      <td>0.881041</td>\n",
       "      <td>-0.010155</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                AtBat      Hits     HmRun      Runs       RBI     Walks  \\\n",
       "AtBat        1.000000  0.967939  0.592198  0.913060  0.820539  0.669845   \n",
       "Hits         0.967939  1.000000  0.562158  0.922187  0.811073  0.641211   \n",
       "HmRun        0.592198  0.562158  1.000000  0.650988  0.855122  0.481014   \n",
       "Runs         0.913060  0.922187  0.650988  1.000000  0.798206  0.732213   \n",
       "RBI          0.820539  0.811073  0.855122  0.798206  1.000000  0.615997   \n",
       "Walks        0.669845  0.641211  0.481014  0.732213  0.615997  1.000000   \n",
       "Years        0.047372  0.044767  0.116318  0.004541  0.146168  0.136475   \n",
       "CAtBat       0.235526  0.227565  0.221882  0.186497  0.294688  0.277175   \n",
       "CHits        0.252717  0.255815  0.220627  0.204830  0.308201  0.280671   \n",
       "CHmRun       0.236659  0.202712  0.493227  0.227913  0.441771  0.332473   \n",
       "CRuns        0.266534  0.261787  0.262361  0.250556  0.323285  0.338478   \n",
       "CRBI         0.244053  0.232005  0.351979  0.205976  0.393184  0.308631   \n",
       "CWalks       0.166123  0.151818  0.233154  0.182168  0.250914  0.424507   \n",
       "PutOuts      0.317550  0.310673  0.282923  0.279347  0.343186  0.299515   \n",
       "Assists      0.353824  0.320455 -0.106329  0.220567  0.106591  0.149656   \n",
       "Errors       0.352117  0.310038  0.039318  0.240475  0.193370  0.129382   \n",
       "Salary       0.394771  0.438675  0.343028  0.419859  0.449457  0.443867   \n",
       "League_N    -0.101663 -0.101038 -0.177258 -0.150392 -0.145597 -0.045337   \n",
       "Division_W  -0.045441 -0.071143 -0.017206 -0.076311 -0.075531 -0.059086   \n",
       "NewLeague_N -0.057655 -0.060608 -0.166044 -0.115446 -0.120124 -0.020029   \n",
       "\n",
       "                Years    CAtBat     CHits    CHmRun     CRuns      CRBI  \\\n",
       "AtBat        0.047372  0.235526  0.252717  0.236659  0.266534  0.244053   \n",
       "Hits         0.044767  0.227565  0.255815  0.202712  0.261787  0.232005   \n",
       "HmRun        0.116318  0.221882  0.220627  0.493227  0.262361  0.351979   \n",
       "Runs         0.004541  0.186497  0.204830  0.227913  0.250556  0.205976   \n",
       "RBI          0.146168  0.294688  0.308201  0.441771  0.323285  0.393184   \n",
       "Walks        0.136475  0.277175  0.280671  0.332473  0.338478  0.308631   \n",
       "Years        1.000000  0.920289  0.903631  0.726872  0.882877  0.868812   \n",
       "CAtBat       0.920289  1.000000  0.995063  0.798836  0.983345  0.949219   \n",
       "CHits        0.903631  0.995063  1.000000  0.783306  0.984609  0.945141   \n",
       "CHmRun       0.726872  0.798836  0.783306  1.000000  0.820243  0.929484   \n",
       "CRuns        0.882877  0.983345  0.984609  0.820243  1.000000  0.943769   \n",
       "CRBI         0.868812  0.949219  0.945141  0.929484  0.943769  1.000000   \n",
       "CWalks       0.838533  0.906501  0.890954  0.799983  0.927807  0.884726   \n",
       "PutOuts     -0.004684  0.062283  0.076547  0.112724  0.064180  0.110098   \n",
       "Assists     -0.080638  0.002038 -0.002523 -0.158511 -0.022978 -0.079387   \n",
       "Errors      -0.162140 -0.066922 -0.062756 -0.138115 -0.084395 -0.100990   \n",
       "Salary       0.400657  0.526135  0.548910  0.524931  0.562678  0.566966   \n",
       "League_N    -0.047261 -0.028738 -0.025790 -0.098383 -0.054666 -0.052684   \n",
       "Division_W   0.021126  0.022965  0.013584  0.006783 -0.000935  0.013213   \n",
       "NewLeague_N -0.042919 -0.016502 -0.010047 -0.097461 -0.042882 -0.045737   \n",
       "\n",
       "               CWalks   PutOuts   Assists    Errors    Salary  League_N  \\\n",
       "AtBat        0.166123  0.317550  0.353824  0.352117  0.394771 -0.101663   \n",
       "Hits         0.151818  0.310673  0.320455  0.310038  0.438675 -0.101038   \n",
       "HmRun        0.233154  0.282923 -0.106329  0.039318  0.343028 -0.177258   \n",
       "Runs         0.182168  0.279347  0.220567  0.240475  0.419859 -0.150392   \n",
       "RBI          0.250914  0.343186  0.106591  0.193370  0.449457 -0.145597   \n",
       "Walks        0.424507  0.299515  0.149656  0.129382  0.443867 -0.045337   \n",
       "Years        0.838533 -0.004684 -0.080638 -0.162140  0.400657 -0.047261   \n",
       "CAtBat       0.906501  0.062283  0.002038 -0.066922  0.526135 -0.028738   \n",
       "CHits        0.890954  0.076547 -0.002523 -0.062756  0.548910 -0.025790   \n",
       "CHmRun       0.799983  0.112724 -0.158511 -0.138115  0.524931 -0.098383   \n",
       "CRuns        0.927807  0.064180 -0.022978 -0.084395  0.562678 -0.054666   \n",
       "CRBI         0.884726  0.110098 -0.079387 -0.100990  0.566966 -0.052684   \n",
       "CWalks       1.000000  0.058638 -0.039130 -0.118475  0.489822 -0.050884   \n",
       "PutOuts      0.058638  1.000000 -0.025024  0.109972  0.300480  0.023984   \n",
       "Assists     -0.039130 -0.025024  1.000000  0.706362  0.025436  0.045527   \n",
       "Errors      -0.118475  0.109972  0.706362  1.000000 -0.005401  0.078508   \n",
       "Salary       0.489822  0.300480  0.025436 -0.005401  1.000000 -0.014282   \n",
       "League_N    -0.050884  0.023984  0.045527  0.078508 -0.014282  1.000000   \n",
       "Division_W  -0.005183 -0.005647 -0.013852 -0.023121 -0.192514 -0.004067   \n",
       "NewLeague_N -0.051726  0.032833  0.039066  0.050140 -0.002834  0.881041   \n",
       "\n",
       "             Division_W  NewLeague_N  \n",
       "AtBat         -0.045441    -0.057655  \n",
       "Hits          -0.071143    -0.060608  \n",
       "HmRun         -0.017206    -0.166044  \n",
       "Runs          -0.076311    -0.115446  \n",
       "RBI           -0.075531    -0.120124  \n",
       "Walks         -0.059086    -0.020029  \n",
       "Years          0.021126    -0.042919  \n",
       "CAtBat         0.022965    -0.016502  \n",
       "CHits          0.013584    -0.010047  \n",
       "CHmRun         0.006783    -0.097461  \n",
       "CRuns         -0.000935    -0.042882  \n",
       "CRBI           0.013213    -0.045737  \n",
       "CWalks        -0.005183    -0.051726  \n",
       "PutOuts       -0.005647     0.032833  \n",
       "Assists       -0.013852     0.039066  \n",
       "Errors        -0.023121     0.050140  \n",
       "Salary        -0.192514    -0.002834  \n",
       "League_N      -0.004067     0.881041  \n",
       "Division_W     1.000000    -0.010155  \n",
       "NewLeague_N   -0.010155     1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop(['Errors','Assists','League_N','NewLeague_N'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Division_W</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>69</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>632</td>\n",
       "      <td>475.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>63</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>880</td>\n",
       "      <td>480.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>225</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>200</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>805</td>\n",
       "      <td>91.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>497</td>\n",
       "      <td>127</td>\n",
       "      <td>7</td>\n",
       "      <td>65</td>\n",
       "      <td>48</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>2703</td>\n",
       "      <td>806</td>\n",
       "      <td>32</td>\n",
       "      <td>379</td>\n",
       "      <td>311</td>\n",
       "      <td>138</td>\n",
       "      <td>325</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>492</td>\n",
       "      <td>136</td>\n",
       "      <td>5</td>\n",
       "      <td>76</td>\n",
       "      <td>50</td>\n",
       "      <td>94</td>\n",
       "      <td>12</td>\n",
       "      <td>5511</td>\n",
       "      <td>1511</td>\n",
       "      <td>39</td>\n",
       "      <td>897</td>\n",
       "      <td>451</td>\n",
       "      <td>875</td>\n",
       "      <td>313</td>\n",
       "      <td>875.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>475</td>\n",
       "      <td>126</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>43</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>1700</td>\n",
       "      <td>433</td>\n",
       "      <td>7</td>\n",
       "      <td>217</td>\n",
       "      <td>93</td>\n",
       "      <td>146</td>\n",
       "      <td>37</td>\n",
       "      <td>385.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>573</td>\n",
       "      <td>144</td>\n",
       "      <td>9</td>\n",
       "      <td>85</td>\n",
       "      <td>60</td>\n",
       "      <td>78</td>\n",
       "      <td>8</td>\n",
       "      <td>3198</td>\n",
       "      <td>857</td>\n",
       "      <td>97</td>\n",
       "      <td>470</td>\n",
       "      <td>420</td>\n",
       "      <td>332</td>\n",
       "      <td>1314</td>\n",
       "      <td>960.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>631</td>\n",
       "      <td>170</td>\n",
       "      <td>9</td>\n",
       "      <td>77</td>\n",
       "      <td>44</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "      <td>4908</td>\n",
       "      <td>1457</td>\n",
       "      <td>30</td>\n",
       "      <td>775</td>\n",
       "      <td>357</td>\n",
       "      <td>249</td>\n",
       "      <td>408</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>322 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  \\\n",
       "0      293    66      1    30   29     14      1     293     66       1   \n",
       "1      315    81      7    24   38     39     14    3449    835      69   \n",
       "2      479   130     18    66   72     76      3    1624    457      63   \n",
       "3      496   141     20    65   78     37     11    5628   1575     225   \n",
       "4      321    87     10    39   42     30      2     396    101      12   \n",
       "..     ...   ...    ...   ...  ...    ...    ...     ...    ...     ...   \n",
       "317    497   127      7    65   48     37      5    2703    806      32   \n",
       "318    492   136      5    76   50     94     12    5511   1511      39   \n",
       "319    475   126      3    61   43     52      6    1700    433       7   \n",
       "320    573   144      9    85   60     78      8    3198    857      97   \n",
       "321    631   170      9    77   44     31     11    4908   1457      30   \n",
       "\n",
       "     CRuns  CRBI  CWalks  PutOuts  Salary  Division_W  \n",
       "0       30    29      14      446     NaN           0  \n",
       "1      321   414     375      632   475.0           1  \n",
       "2      224   266     263      880   480.0           1  \n",
       "3      828   838     354      200   500.0           0  \n",
       "4       48    46      33      805    91.5           0  \n",
       "..     ...   ...     ...      ...     ...         ...  \n",
       "317    379   311     138      325   700.0           0  \n",
       "318    897   451     875      313   875.0           0  \n",
       "319    217    93     146       37   385.0           1  \n",
       "320    470   420     332     1314   960.0           0  \n",
       "321    775   357     249      408  1000.0           1  \n",
       "\n",
       "[322 rows x 16 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Division_W</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AtBat</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967939</td>\n",
       "      <td>0.592198</td>\n",
       "      <td>0.913060</td>\n",
       "      <td>0.820539</td>\n",
       "      <td>0.669845</td>\n",
       "      <td>0.047372</td>\n",
       "      <td>0.235526</td>\n",
       "      <td>0.252717</td>\n",
       "      <td>0.236659</td>\n",
       "      <td>0.266534</td>\n",
       "      <td>0.244053</td>\n",
       "      <td>0.166123</td>\n",
       "      <td>0.317550</td>\n",
       "      <td>0.394771</td>\n",
       "      <td>-0.045441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hits</th>\n",
       "      <td>0.967939</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.562158</td>\n",
       "      <td>0.922187</td>\n",
       "      <td>0.811073</td>\n",
       "      <td>0.641211</td>\n",
       "      <td>0.044767</td>\n",
       "      <td>0.227565</td>\n",
       "      <td>0.255815</td>\n",
       "      <td>0.202712</td>\n",
       "      <td>0.261787</td>\n",
       "      <td>0.232005</td>\n",
       "      <td>0.151818</td>\n",
       "      <td>0.310673</td>\n",
       "      <td>0.438675</td>\n",
       "      <td>-0.071143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HmRun</th>\n",
       "      <td>0.592198</td>\n",
       "      <td>0.562158</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.650988</td>\n",
       "      <td>0.855122</td>\n",
       "      <td>0.481014</td>\n",
       "      <td>0.116318</td>\n",
       "      <td>0.221882</td>\n",
       "      <td>0.220627</td>\n",
       "      <td>0.493227</td>\n",
       "      <td>0.262361</td>\n",
       "      <td>0.351979</td>\n",
       "      <td>0.233154</td>\n",
       "      <td>0.282923</td>\n",
       "      <td>0.343028</td>\n",
       "      <td>-0.017206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Runs</th>\n",
       "      <td>0.913060</td>\n",
       "      <td>0.922187</td>\n",
       "      <td>0.650988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.798206</td>\n",
       "      <td>0.732213</td>\n",
       "      <td>0.004541</td>\n",
       "      <td>0.186497</td>\n",
       "      <td>0.204830</td>\n",
       "      <td>0.227913</td>\n",
       "      <td>0.250556</td>\n",
       "      <td>0.205976</td>\n",
       "      <td>0.182168</td>\n",
       "      <td>0.279347</td>\n",
       "      <td>0.419859</td>\n",
       "      <td>-0.076311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBI</th>\n",
       "      <td>0.820539</td>\n",
       "      <td>0.811073</td>\n",
       "      <td>0.855122</td>\n",
       "      <td>0.798206</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615997</td>\n",
       "      <td>0.146168</td>\n",
       "      <td>0.294688</td>\n",
       "      <td>0.308201</td>\n",
       "      <td>0.441771</td>\n",
       "      <td>0.323285</td>\n",
       "      <td>0.393184</td>\n",
       "      <td>0.250914</td>\n",
       "      <td>0.343186</td>\n",
       "      <td>0.449457</td>\n",
       "      <td>-0.075531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Walks</th>\n",
       "      <td>0.669845</td>\n",
       "      <td>0.641211</td>\n",
       "      <td>0.481014</td>\n",
       "      <td>0.732213</td>\n",
       "      <td>0.615997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.136475</td>\n",
       "      <td>0.277175</td>\n",
       "      <td>0.280671</td>\n",
       "      <td>0.332473</td>\n",
       "      <td>0.338478</td>\n",
       "      <td>0.308631</td>\n",
       "      <td>0.424507</td>\n",
       "      <td>0.299515</td>\n",
       "      <td>0.443867</td>\n",
       "      <td>-0.059086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Years</th>\n",
       "      <td>0.047372</td>\n",
       "      <td>0.044767</td>\n",
       "      <td>0.116318</td>\n",
       "      <td>0.004541</td>\n",
       "      <td>0.146168</td>\n",
       "      <td>0.136475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920289</td>\n",
       "      <td>0.903631</td>\n",
       "      <td>0.726872</td>\n",
       "      <td>0.882877</td>\n",
       "      <td>0.868812</td>\n",
       "      <td>0.838533</td>\n",
       "      <td>-0.004684</td>\n",
       "      <td>0.400657</td>\n",
       "      <td>0.021126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAtBat</th>\n",
       "      <td>0.235526</td>\n",
       "      <td>0.227565</td>\n",
       "      <td>0.221882</td>\n",
       "      <td>0.186497</td>\n",
       "      <td>0.294688</td>\n",
       "      <td>0.277175</td>\n",
       "      <td>0.920289</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995063</td>\n",
       "      <td>0.798836</td>\n",
       "      <td>0.983345</td>\n",
       "      <td>0.949219</td>\n",
       "      <td>0.906501</td>\n",
       "      <td>0.062283</td>\n",
       "      <td>0.526135</td>\n",
       "      <td>0.022965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHits</th>\n",
       "      <td>0.252717</td>\n",
       "      <td>0.255815</td>\n",
       "      <td>0.220627</td>\n",
       "      <td>0.204830</td>\n",
       "      <td>0.308201</td>\n",
       "      <td>0.280671</td>\n",
       "      <td>0.903631</td>\n",
       "      <td>0.995063</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.783306</td>\n",
       "      <td>0.984609</td>\n",
       "      <td>0.945141</td>\n",
       "      <td>0.890954</td>\n",
       "      <td>0.076547</td>\n",
       "      <td>0.548910</td>\n",
       "      <td>0.013584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHmRun</th>\n",
       "      <td>0.236659</td>\n",
       "      <td>0.202712</td>\n",
       "      <td>0.493227</td>\n",
       "      <td>0.227913</td>\n",
       "      <td>0.441771</td>\n",
       "      <td>0.332473</td>\n",
       "      <td>0.726872</td>\n",
       "      <td>0.798836</td>\n",
       "      <td>0.783306</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.820243</td>\n",
       "      <td>0.929484</td>\n",
       "      <td>0.799983</td>\n",
       "      <td>0.112724</td>\n",
       "      <td>0.524931</td>\n",
       "      <td>0.006783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRuns</th>\n",
       "      <td>0.266534</td>\n",
       "      <td>0.261787</td>\n",
       "      <td>0.262361</td>\n",
       "      <td>0.250556</td>\n",
       "      <td>0.323285</td>\n",
       "      <td>0.338478</td>\n",
       "      <td>0.882877</td>\n",
       "      <td>0.983345</td>\n",
       "      <td>0.984609</td>\n",
       "      <td>0.820243</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.943769</td>\n",
       "      <td>0.927807</td>\n",
       "      <td>0.064180</td>\n",
       "      <td>0.562678</td>\n",
       "      <td>-0.000935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRBI</th>\n",
       "      <td>0.244053</td>\n",
       "      <td>0.232005</td>\n",
       "      <td>0.351979</td>\n",
       "      <td>0.205976</td>\n",
       "      <td>0.393184</td>\n",
       "      <td>0.308631</td>\n",
       "      <td>0.868812</td>\n",
       "      <td>0.949219</td>\n",
       "      <td>0.945141</td>\n",
       "      <td>0.929484</td>\n",
       "      <td>0.943769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884726</td>\n",
       "      <td>0.110098</td>\n",
       "      <td>0.566966</td>\n",
       "      <td>0.013213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWalks</th>\n",
       "      <td>0.166123</td>\n",
       "      <td>0.151818</td>\n",
       "      <td>0.233154</td>\n",
       "      <td>0.182168</td>\n",
       "      <td>0.250914</td>\n",
       "      <td>0.424507</td>\n",
       "      <td>0.838533</td>\n",
       "      <td>0.906501</td>\n",
       "      <td>0.890954</td>\n",
       "      <td>0.799983</td>\n",
       "      <td>0.927807</td>\n",
       "      <td>0.884726</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058638</td>\n",
       "      <td>0.489822</td>\n",
       "      <td>-0.005183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PutOuts</th>\n",
       "      <td>0.317550</td>\n",
       "      <td>0.310673</td>\n",
       "      <td>0.282923</td>\n",
       "      <td>0.279347</td>\n",
       "      <td>0.343186</td>\n",
       "      <td>0.299515</td>\n",
       "      <td>-0.004684</td>\n",
       "      <td>0.062283</td>\n",
       "      <td>0.076547</td>\n",
       "      <td>0.112724</td>\n",
       "      <td>0.064180</td>\n",
       "      <td>0.110098</td>\n",
       "      <td>0.058638</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300480</td>\n",
       "      <td>-0.005647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Salary</th>\n",
       "      <td>0.394771</td>\n",
       "      <td>0.438675</td>\n",
       "      <td>0.343028</td>\n",
       "      <td>0.419859</td>\n",
       "      <td>0.449457</td>\n",
       "      <td>0.443867</td>\n",
       "      <td>0.400657</td>\n",
       "      <td>0.526135</td>\n",
       "      <td>0.548910</td>\n",
       "      <td>0.524931</td>\n",
       "      <td>0.562678</td>\n",
       "      <td>0.566966</td>\n",
       "      <td>0.489822</td>\n",
       "      <td>0.300480</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.192514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Division_W</th>\n",
       "      <td>-0.045441</td>\n",
       "      <td>-0.071143</td>\n",
       "      <td>-0.017206</td>\n",
       "      <td>-0.076311</td>\n",
       "      <td>-0.075531</td>\n",
       "      <td>-0.059086</td>\n",
       "      <td>0.021126</td>\n",
       "      <td>0.022965</td>\n",
       "      <td>0.013584</td>\n",
       "      <td>0.006783</td>\n",
       "      <td>-0.000935</td>\n",
       "      <td>0.013213</td>\n",
       "      <td>-0.005183</td>\n",
       "      <td>-0.005647</td>\n",
       "      <td>-0.192514</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               AtBat      Hits     HmRun      Runs       RBI     Walks  \\\n",
       "AtBat       1.000000  0.967939  0.592198  0.913060  0.820539  0.669845   \n",
       "Hits        0.967939  1.000000  0.562158  0.922187  0.811073  0.641211   \n",
       "HmRun       0.592198  0.562158  1.000000  0.650988  0.855122  0.481014   \n",
       "Runs        0.913060  0.922187  0.650988  1.000000  0.798206  0.732213   \n",
       "RBI         0.820539  0.811073  0.855122  0.798206  1.000000  0.615997   \n",
       "Walks       0.669845  0.641211  0.481014  0.732213  0.615997  1.000000   \n",
       "Years       0.047372  0.044767  0.116318  0.004541  0.146168  0.136475   \n",
       "CAtBat      0.235526  0.227565  0.221882  0.186497  0.294688  0.277175   \n",
       "CHits       0.252717  0.255815  0.220627  0.204830  0.308201  0.280671   \n",
       "CHmRun      0.236659  0.202712  0.493227  0.227913  0.441771  0.332473   \n",
       "CRuns       0.266534  0.261787  0.262361  0.250556  0.323285  0.338478   \n",
       "CRBI        0.244053  0.232005  0.351979  0.205976  0.393184  0.308631   \n",
       "CWalks      0.166123  0.151818  0.233154  0.182168  0.250914  0.424507   \n",
       "PutOuts     0.317550  0.310673  0.282923  0.279347  0.343186  0.299515   \n",
       "Salary      0.394771  0.438675  0.343028  0.419859  0.449457  0.443867   \n",
       "Division_W -0.045441 -0.071143 -0.017206 -0.076311 -0.075531 -0.059086   \n",
       "\n",
       "               Years    CAtBat     CHits    CHmRun     CRuns      CRBI  \\\n",
       "AtBat       0.047372  0.235526  0.252717  0.236659  0.266534  0.244053   \n",
       "Hits        0.044767  0.227565  0.255815  0.202712  0.261787  0.232005   \n",
       "HmRun       0.116318  0.221882  0.220627  0.493227  0.262361  0.351979   \n",
       "Runs        0.004541  0.186497  0.204830  0.227913  0.250556  0.205976   \n",
       "RBI         0.146168  0.294688  0.308201  0.441771  0.323285  0.393184   \n",
       "Walks       0.136475  0.277175  0.280671  0.332473  0.338478  0.308631   \n",
       "Years       1.000000  0.920289  0.903631  0.726872  0.882877  0.868812   \n",
       "CAtBat      0.920289  1.000000  0.995063  0.798836  0.983345  0.949219   \n",
       "CHits       0.903631  0.995063  1.000000  0.783306  0.984609  0.945141   \n",
       "CHmRun      0.726872  0.798836  0.783306  1.000000  0.820243  0.929484   \n",
       "CRuns       0.882877  0.983345  0.984609  0.820243  1.000000  0.943769   \n",
       "CRBI        0.868812  0.949219  0.945141  0.929484  0.943769  1.000000   \n",
       "CWalks      0.838533  0.906501  0.890954  0.799983  0.927807  0.884726   \n",
       "PutOuts    -0.004684  0.062283  0.076547  0.112724  0.064180  0.110098   \n",
       "Salary      0.400657  0.526135  0.548910  0.524931  0.562678  0.566966   \n",
       "Division_W  0.021126  0.022965  0.013584  0.006783 -0.000935  0.013213   \n",
       "\n",
       "              CWalks   PutOuts    Salary  Division_W  \n",
       "AtBat       0.166123  0.317550  0.394771   -0.045441  \n",
       "Hits        0.151818  0.310673  0.438675   -0.071143  \n",
       "HmRun       0.233154  0.282923  0.343028   -0.017206  \n",
       "Runs        0.182168  0.279347  0.419859   -0.076311  \n",
       "RBI         0.250914  0.343186  0.449457   -0.075531  \n",
       "Walks       0.424507  0.299515  0.443867   -0.059086  \n",
       "Years       0.838533 -0.004684  0.400657    0.021126  \n",
       "CAtBat      0.906501  0.062283  0.526135    0.022965  \n",
       "CHits       0.890954  0.076547  0.548910    0.013584  \n",
       "CHmRun      0.799983  0.112724  0.524931    0.006783  \n",
       "CRuns       0.927807  0.064180  0.562678   -0.000935  \n",
       "CRBI        0.884726  0.110098  0.566966    0.013213  \n",
       "CWalks      1.000000  0.058638  0.489822   -0.005183  \n",
       "PutOuts     0.058638  1.000000  0.300480   -0.005647  \n",
       "Salary      0.489822  0.300480  1.000000   -0.192514  \n",
       "Division_W -0.005183 -0.005647 -0.192514    1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 322 entries, 0 to 321\n",
      "Data columns (total 20 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   AtBat        322 non-null    int64  \n",
      " 1   Hits         322 non-null    int64  \n",
      " 2   HmRun        322 non-null    int64  \n",
      " 3   Runs         322 non-null    int64  \n",
      " 4   RBI          322 non-null    int64  \n",
      " 5   Walks        322 non-null    int64  \n",
      " 6   Years        322 non-null    int64  \n",
      " 7   CAtBat       322 non-null    int64  \n",
      " 8   CHits        322 non-null    int64  \n",
      " 9   CHmRun       322 non-null    int64  \n",
      " 10  CRuns        322 non-null    int64  \n",
      " 11  CRBI         322 non-null    int64  \n",
      " 12  CWalks       322 non-null    int64  \n",
      " 13  PutOuts      322 non-null    int64  \n",
      " 14  Assists      322 non-null    int64  \n",
      " 15  Errors       322 non-null    int64  \n",
      " 16  Salary       263 non-null    float64\n",
      " 17  League_N     322 non-null    uint8  \n",
      " 18  Division_W   322 non-null    uint8  \n",
      " 19  NewLeague_N  322 non-null    uint8  \n",
      "dtypes: float64(1), int64(16), uint8(3)\n",
      "memory usage: 43.8 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Analysis (Just look)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>446</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>69</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>632</td>\n",
       "      <td>475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>63</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>880</td>\n",
       "      <td>480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>225</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>200</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>805</td>\n",
       "      <td>91.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n",
       "0    293    66      1    30   29     14      1     293     66       1     30   \n",
       "1    315    81      7    24   38     39     14    3449    835      69    321   \n",
       "2    479   130     18    66   72     76      3    1624    457      63    224   \n",
       "3    496   141     20    65   78     37     11    5628   1575     225    828   \n",
       "4    321    87     10    39   42     30      2     396    101      12     48   \n",
       "\n",
       "   CRBI  CWalks  PutOuts  Salary  \n",
       "0    29      14      446     NaN  \n",
       "1   414     375      632   475.0  \n",
       "2   266     263      880   480.0  \n",
       "3   838     354      200   500.0  \n",
       "4    46      33      805    91.5  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df1.select_dtypes(include = ['float64', 'int64']) \n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df1_salary = df1[df1['Years']==3]['Salary']\n",
    "#sns.boxplot(x = df1_salary);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reindex from a duplicate axis",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-bc03b95eb9cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mlower\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQ1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mIQR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mdf1_salary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1_salary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mupper\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf1_salary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"yes\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1_salary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1_salary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mupper\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf1_salary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36mcheck_bool_indexer\u001b[1;34m(index, key)\u001b[0m\n\u001b[0;32m   2311\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2312\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2313\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2314\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2315\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mreindex\u001b[1;34m(self, index, **kwargs)\u001b[0m\n\u001b[0;32m   4028\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4029\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4030\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4032\u001b[0m     def drop(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mreindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4542\u001b[0m         \u001b[1;31m# perform the reindex on the axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4543\u001b[0m         return self._reindex_axes(\n\u001b[1;32m-> 4544\u001b[1;33m             \u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4545\u001b[0m         ).__finalize__(self)\n\u001b[0;32m   4546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_reindex_axes\u001b[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[0;32m   4565\u001b[0m                 \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4566\u001b[0m                 \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4567\u001b[1;33m                 \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4568\u001b[0m             )\n\u001b[0;32m   4569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[1;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[0;32m   4611\u001b[0m                 \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4612\u001b[0m                 \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_dups\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4613\u001b[1;33m                 \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4614\u001b[0m             )\n\u001b[0;32m   4615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \u001b[1;31m# some axes don't allow reindexing with dups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1251\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_reindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_can_reindex\u001b[1;34m(self, indexer)\u001b[0m\n\u001b[0;32m   3097\u001b[0m         \u001b[1;31m# trying to reindex on an axis with duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3098\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3099\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot reindex from a duplicate axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reindex from a duplicate axis"
     ]
    }
   ],
   "source": [
    "# Aykiri Gozlem Yapmaya Ihtiyac Yoktur\n",
    "#for feature in [df1_salary]:\n",
    "\n",
    "    Q1 = df1_salary[feature].quantile(0.25)\n",
    "    Q3 = df1_salary[feature].quantile(0.75)\n",
    "    IQR = Q3-Q1\n",
    "    upper = Q3 + 1.5*IQR\n",
    "    lower = Q1 - 1.5*IQR\n",
    "\n",
    "    if df1_salary[(df1_salary[feature] > upper) | (df1_salary[feature] < lower)].any(axis=None):\n",
    "        print(feature,\"yes\")\n",
    "        print(df1_salary[(df1_salary[feature] > upper) | (df1_salary[feature] < lower)].shape[0])\n",
    "    else:\n",
    "        print(feature, \"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salary yes\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Aykiri Gozlem Yapmaya Ihtiyac Yoktur\n",
    "#for feature in [\"Salary\"]:\n",
    "\n",
    "    Q1 = df1[feature].quantile(0.25)\n",
    "    Q3 = df1[feature].quantile(0.75)\n",
    "    IQR = Q3-Q1\n",
    "    upper = Q3 + 1.5*IQR\n",
    "    lower = Q1 - 1.5*IQR\n",
    "\n",
    "    if df1[(df1[feature] > upper) | (df1[feature] < lower)].any(axis=None):\n",
    "        print(feature,\"yes\")\n",
    "        print(df1[(df1[feature] > upper) | (df1[feature] < lower)].shape[0])\n",
    "    else:\n",
    "        print(feature, \"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Years\n",
       "1      920.000\n",
       "2     2127.333\n",
       "3      480.000\n",
       "4     1220.000\n",
       "5     1975.000\n",
       "6     1350.000\n",
       "7     1183.333\n",
       "8     1670.000\n",
       "9     1940.000\n",
       "10    2460.000\n",
       "11    1900.000\n",
       "12    1300.000\n",
       "13    2412.500\n",
       "14    1861.460\n",
       "15     933.333\n",
       "16    1050.000\n",
       "17     950.000\n",
       "18    1450.000\n",
       "19     500.000\n",
       "20     487.500\n",
       "23         NaN\n",
       "24     750.000\n",
       "Name: Salary, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.groupby('Years').Salary.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Years</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>206.039331</td>\n",
       "      <td>404.082468</td>\n",
       "      <td>162.914905</td>\n",
       "      <td>335.916825</td>\n",
       "      <td>559.885735</td>\n",
       "      <td>524.755366</td>\n",
       "      <td>567.715565</td>\n",
       "      <td>600.041808</td>\n",
       "      <td>721.008265</td>\n",
       "      <td>708.129992</td>\n",
       "      <td>724.489063</td>\n",
       "      <td>512.862951</td>\n",
       "      <td>955.293317</td>\n",
       "      <td>722.553985</td>\n",
       "      <td>479.872771</td>\n",
       "      <td>476.284774</td>\n",
       "      <td>514.850896</td>\n",
       "      <td>580.434500</td>\n",
       "      <td>428.714286</td>\n",
       "      <td>283.474150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>643.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>293.766001</td>\n",
       "      <td>708.174665</td>\n",
       "      <td>140.035071</td>\n",
       "      <td>373.567746</td>\n",
       "      <td>611.952264</td>\n",
       "      <td>430.754287</td>\n",
       "      <td>373.602121</td>\n",
       "      <td>522.941972</td>\n",
       "      <td>600.640653</td>\n",
       "      <td>759.685440</td>\n",
       "      <td>550.932019</td>\n",
       "      <td>387.930549</td>\n",
       "      <td>772.840480</td>\n",
       "      <td>568.853944</td>\n",
       "      <td>294.581287</td>\n",
       "      <td>328.748070</td>\n",
       "      <td>340.117786</td>\n",
       "      <td>423.450942</td>\n",
       "      <td>188.604272</td>\n",
       "      <td>150.767516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>283.09539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1%</th>\n",
       "      <td>20.570000</td>\n",
       "      <td>24.255000</td>\n",
       "      <td>27.570000</td>\n",
       "      <td>32.165000</td>\n",
       "      <td>29.780000</td>\n",
       "      <td>32.340000</td>\n",
       "      <td>36.131332</td>\n",
       "      <td>19.680000</td>\n",
       "      <td>23.410000</td>\n",
       "      <td>18.160000</td>\n",
       "      <td>38.940000</td>\n",
       "      <td>25.050000</td>\n",
       "      <td>26.100000</td>\n",
       "      <td>28.660000</td>\n",
       "      <td>21.407126</td>\n",
       "      <td>21.920000</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>30.940000</td>\n",
       "      <td>15.860000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.94000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5%</th>\n",
       "      <td>34.850000</td>\n",
       "      <td>37.275000</td>\n",
       "      <td>41.850000</td>\n",
       "      <td>48.825000</td>\n",
       "      <td>44.900000</td>\n",
       "      <td>49.700000</td>\n",
       "      <td>104.656658</td>\n",
       "      <td>54.400000</td>\n",
       "      <td>69.050000</td>\n",
       "      <td>42.800000</td>\n",
       "      <td>162.700000</td>\n",
       "      <td>85.250000</td>\n",
       "      <td>90.500000</td>\n",
       "      <td>95.300000</td>\n",
       "      <td>79.035632</td>\n",
       "      <td>93.600000</td>\n",
       "      <td>85.500000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>150.700000</td>\n",
       "      <td>71.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>225.70000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>108.750000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>244.399443</td>\n",
       "      <td>430.865057</td>\n",
       "      <td>483.078016</td>\n",
       "      <td>600.745605</td>\n",
       "      <td>502.792120</td>\n",
       "      <td>626.875000</td>\n",
       "      <td>611.777843</td>\n",
       "      <td>621.875000</td>\n",
       "      <td>463.333375</td>\n",
       "      <td>805.394719</td>\n",
       "      <td>663.772274</td>\n",
       "      <td>482.500000</td>\n",
       "      <td>433.139095</td>\n",
       "      <td>480.237084</td>\n",
       "      <td>494.737799</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>307.812500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>750.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>162.527191</td>\n",
       "      <td>272.750328</td>\n",
       "      <td>180.375000</td>\n",
       "      <td>330.714286</td>\n",
       "      <td>590.704327</td>\n",
       "      <td>676.977670</td>\n",
       "      <td>771.875000</td>\n",
       "      <td>748.750045</td>\n",
       "      <td>900.364500</td>\n",
       "      <td>720.550625</td>\n",
       "      <td>848.828125</td>\n",
       "      <td>625.437525</td>\n",
       "      <td>1245.542825</td>\n",
       "      <td>890.299625</td>\n",
       "      <td>639.791554</td>\n",
       "      <td>625.312500</td>\n",
       "      <td>798.749750</td>\n",
       "      <td>717.166800</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>361.718750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>750.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>416.817312</td>\n",
       "      <td>946.017518</td>\n",
       "      <td>286.625000</td>\n",
       "      <td>611.000000</td>\n",
       "      <td>1050.562500</td>\n",
       "      <td>990.375000</td>\n",
       "      <td>941.249900</td>\n",
       "      <td>1160.750000</td>\n",
       "      <td>1339.749825</td>\n",
       "      <td>1271.750000</td>\n",
       "      <td>1191.250000</td>\n",
       "      <td>896.625000</td>\n",
       "      <td>1861.250000</td>\n",
       "      <td>1265.729725</td>\n",
       "      <td>807.916450</td>\n",
       "      <td>866.250000</td>\n",
       "      <td>897.500000</td>\n",
       "      <td>978.666900</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>437.187500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>750.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>668.408656</td>\n",
       "      <td>1536.675259</td>\n",
       "      <td>383.312500</td>\n",
       "      <td>915.500000</td>\n",
       "      <td>1512.781250</td>\n",
       "      <td>1170.187500</td>\n",
       "      <td>1062.291450</td>\n",
       "      <td>1415.375000</td>\n",
       "      <td>1639.874912</td>\n",
       "      <td>1865.875000</td>\n",
       "      <td>1545.625000</td>\n",
       "      <td>1098.312500</td>\n",
       "      <td>2136.875000</td>\n",
       "      <td>1563.594862</td>\n",
       "      <td>870.624725</td>\n",
       "      <td>958.125000</td>\n",
       "      <td>923.750000</td>\n",
       "      <td>1214.333450</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>462.343750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>750.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>869.681731</td>\n",
       "      <td>2009.201452</td>\n",
       "      <td>460.662500</td>\n",
       "      <td>1159.100000</td>\n",
       "      <td>1882.556250</td>\n",
       "      <td>1314.037500</td>\n",
       "      <td>1159.124690</td>\n",
       "      <td>1619.075000</td>\n",
       "      <td>1879.974982</td>\n",
       "      <td>2341.175000</td>\n",
       "      <td>1829.125000</td>\n",
       "      <td>1259.662500</td>\n",
       "      <td>2357.375000</td>\n",
       "      <td>1801.886972</td>\n",
       "      <td>920.791345</td>\n",
       "      <td>1031.625000</td>\n",
       "      <td>944.750000</td>\n",
       "      <td>1402.866690</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>482.468750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>750.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>920.000000</td>\n",
       "      <td>2127.333000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>1220.000000</td>\n",
       "      <td>1975.000000</td>\n",
       "      <td>1350.000000</td>\n",
       "      <td>1183.333000</td>\n",
       "      <td>1670.000000</td>\n",
       "      <td>1940.000000</td>\n",
       "      <td>2460.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>2412.500000</td>\n",
       "      <td>1861.460000</td>\n",
       "      <td>933.333000</td>\n",
       "      <td>1050.000000</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>1450.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>487.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>750.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Years          1            2           3            4            5   \\\n",
       "count    8.000000     8.000000    8.000000     8.000000     8.000000   \n",
       "mean   206.039331   404.082468  162.914905   335.916825   559.885735   \n",
       "std    293.766001   708.174665  140.035071   373.567746   611.952264   \n",
       "min     17.000000    21.000000   24.000000    28.000000    26.000000   \n",
       "1%      20.570000    24.255000   27.570000    32.165000    29.780000   \n",
       "5%      34.850000    37.275000   41.850000    48.825000    44.900000   \n",
       "50%    108.750000   135.000000  126.250000   244.399443   430.865057   \n",
       "75%    162.527191   272.750328  180.375000   330.714286   590.704327   \n",
       "90%    416.817312   946.017518  286.625000   611.000000  1050.562500   \n",
       "95%    668.408656  1536.675259  383.312500   915.500000  1512.781250   \n",
       "99%    869.681731  2009.201452  460.662500  1159.100000  1882.556250   \n",
       "max    920.000000  2127.333000  480.000000  1220.000000  1975.000000   \n",
       "\n",
       "Years           6            7            8            9            10  \\\n",
       "count     8.000000     8.000000     8.000000     8.000000     8.000000   \n",
       "mean    524.755366   567.715565   600.041808   721.008265   708.129992   \n",
       "std     430.754287   373.602121   522.941972   600.640653   759.685440   \n",
       "min      28.000000    19.000000    11.000000    12.000000    12.000000   \n",
       "1%       32.340000    36.131332    19.680000    23.410000    18.160000   \n",
       "5%       49.700000   104.656658    54.400000    69.050000    42.800000   \n",
       "50%     483.078016   600.745605   502.792120   626.875000   611.777843   \n",
       "75%     676.977670   771.875000   748.750045   900.364500   720.550625   \n",
       "90%     990.375000   941.249900  1160.750000  1339.749825  1271.750000   \n",
       "95%    1170.187500  1062.291450  1415.375000  1639.874912  1865.875000   \n",
       "99%    1314.037500  1159.124690  1619.075000  1879.974982  2341.175000   \n",
       "max    1350.000000  1183.333000  1670.000000  1940.000000  2460.000000   \n",
       "\n",
       "Years           11           12           13           14          15  \\\n",
       "count     8.000000     8.000000     8.000000     8.000000    8.000000   \n",
       "mean    724.489063   512.862951   955.293317   722.553985  479.872771   \n",
       "std     550.932019   387.930549   772.840480   568.853944  294.581287   \n",
       "min       8.000000    10.000000    10.000000    12.000000    7.000000   \n",
       "1%       38.940000    25.050000    26.100000    28.660000   21.407126   \n",
       "5%      162.700000    85.250000    90.500000    95.300000   79.035632   \n",
       "50%     621.875000   463.333375   805.394719   663.772274  482.500000   \n",
       "75%     848.828125   625.437525  1245.542825   890.299625  639.791554   \n",
       "90%    1191.250000   896.625000  1861.250000  1265.729725  807.916450   \n",
       "95%    1545.625000  1098.312500  2136.875000  1563.594862  870.624725   \n",
       "99%    1829.125000  1259.662500  2357.375000  1801.886972  920.791345   \n",
       "max    1900.000000  1300.000000  2412.500000  1861.460000  933.333000   \n",
       "\n",
       "Years           16          17           18          19          20   23  \\\n",
       "count     8.000000    8.000000     8.000000    7.000000    8.000000  1.0   \n",
       "mean    476.284774  514.850896   580.434500  428.714286  283.474150  0.0   \n",
       "std     328.748070  340.117786   423.450942  188.604272  150.767516  NaN   \n",
       "min       4.000000    5.000000     5.000000    1.000000    2.000000  0.0   \n",
       "1%       21.920000   21.100000    27.400000   30.940000   15.860000  0.0   \n",
       "5%       93.600000   85.500000   117.000000  150.700000   71.300000  0.0   \n",
       "50%     433.139095  480.237084   494.737799  500.000000  307.812500  0.0   \n",
       "75%     625.312500  798.749750   717.166800  500.000000  361.718750  0.0   \n",
       "90%     866.250000  897.500000   978.666900  500.000000  437.187500  0.0   \n",
       "95%     958.125000  923.750000  1214.333450  500.000000  462.343750  0.0   \n",
       "99%    1031.625000  944.750000  1402.866690  500.000000  482.468750  0.0   \n",
       "max    1050.000000  950.000000  1450.000000  500.000000  487.500000  0.0   \n",
       "\n",
       "Years         24  \n",
       "count    7.00000  \n",
       "mean   643.00000  \n",
       "std    283.09539  \n",
       "min      1.00000  \n",
       "1%      45.94000  \n",
       "5%     225.70000  \n",
       "50%    750.00000  \n",
       "75%    750.00000  \n",
       "90%    750.00000  \n",
       "95%    750.00000  \n",
       "99%    750.00000  \n",
       "max    750.00000  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "df1.groupby(['Years']).Salary.describe().T.describe([0.01,.05,.75,.90,.95,.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 322 entries, 0 to 321\n",
      "Data columns (total 15 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   AtBat    322 non-null    int64  \n",
      " 1   Hits     322 non-null    int64  \n",
      " 2   HmRun    322 non-null    int64  \n",
      " 3   Runs     322 non-null    int64  \n",
      " 4   RBI      322 non-null    int64  \n",
      " 5   Walks    322 non-null    int64  \n",
      " 6   Years    322 non-null    int64  \n",
      " 7   CAtBat   322 non-null    int64  \n",
      " 8   CHits    322 non-null    int64  \n",
      " 9   CHmRun   322 non-null    int64  \n",
      " 10  CRuns    322 non-null    int64  \n",
      " 11  CRBI     322 non-null    int64  \n",
      " 12  CWalks   322 non-null    int64  \n",
      " 13  PutOuts  322 non-null    int64  \n",
      " 14  Salary   263 non-null    float64\n",
      "dtypes: float64(1), int64(14)\n",
      "memory usage: 37.9 KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df1[\"Salary\"]\n",
    "X = df1.drop('Salary', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X dataframe'inde Standardizasyon yapilmasi istenirse buraya eklenmeli\n",
    "# cols = X.columns\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size = 0.20, random_state = 46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 14)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 14)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_model = LinearRegression()\n",
    "reg_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.197421475217425"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.29622885,  5.47186031,  4.11896662,  0.76439379, -0.67855726,\n",
       "        2.1000431 , -1.00182854, -0.38886748,  1.66850846,  2.40724389,\n",
       "        0.39230642, -0.78376604, -0.02698124,  0.33231432])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263.86419269989364"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#egitim hatasi\n",
    "np.sqrt(mean_squared_error(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = reg_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494.5781861033382"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test hatasi\n",
    "np.sqrt(mean_squared_error(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = Ridge().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.18876221347091"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.29613011,  5.47121111,  4.11693911,  0.76497996, -0.67783798,\n",
       "        2.09975591, -0.99984126, -0.38888836,  1.66860875,  2.40745455,\n",
       "        0.39224321, -0.78384852, -0.02694852,  0.33231589])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#egitim seti basarisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263.86419282562605"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = ridge_model.predict(X_train)\n",
    "np.sqrt(mean_squared_error(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test seti basarisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494.57852614283036"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = ridge_model.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning - Alpha1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas1 = np.random.randint(0,1100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "ridge_cv = RidgeCV(alphas = alphas1, \n",
    "                   cv=10,\n",
    "                   normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([1007]), cv=10, fit_intercept=True, gcv_mode=None,\n",
       "        normalize=True, scoring=None, store_cv_values=False)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1007"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_tuned = Ridge(alpha = ridge_cv.alpha_, \n",
    "                   normalize = True).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Model RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508.57321883609796"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, ridge_tuned.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning - Alpha2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas2 = 10**np.linspace(10,-2,100)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "ridge_cv = RidgeCV(alphas = alphas2, \n",
    "                   cv=10,\n",
    "                   normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([5.00000000e+09, 3.78231664e+09, 2.86118383e+09, 2.16438064e+09,\n",
       "       1.63727458e+09, 1.23853818e+09, 9.36908711e+08, 7.08737081e+08,\n",
       "       5.36133611e+08, 4.05565415e+08, 3.06795364e+08, 2.32079442e+08,\n",
       "       1.75559587e+08, 1.32804389e+08, 1.00461650e+08, 7.59955541e+07,\n",
       "       5.74878498e+07, 4.34874501e+07, 3.28966612e+07, 2.48851178e+07,\n",
       "       1.88246790e+07, 1.42401793e+0...\n",
       "       1.00461650e+00, 7.59955541e-01, 5.74878498e-01, 4.34874501e-01,\n",
       "       3.28966612e-01, 2.48851178e-01, 1.88246790e-01, 1.42401793e-01,\n",
       "       1.07721735e-01, 8.14875417e-02, 6.16423370e-02, 4.66301673e-02,\n",
       "       3.52740116e-02, 2.66834962e-02, 2.01850863e-02, 1.52692775e-02,\n",
       "       1.15506485e-02, 8.73764200e-03, 6.60970574e-03, 5.00000000e-03]),\n",
       "        cv=10, fit_intercept=True, gcv_mode=None, normalize=True, scoring=None,\n",
       "        store_cv_values=False)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7599555414764666"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_tuned = Ridge(alpha = ridge_cv.alpha_, \n",
    "                   normalize = True).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Model RMSE - Alpha2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471.73625496272984"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, ridge_tuned.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning - Alpha3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas3 = np.linspace(0,1,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "ridge_cv = RidgeCV(alphas = alphas3, \n",
    "                   cv=10,\n",
    "                   normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([0.        , 0.001001  , 0.002002  , 0.003003  , 0.004004  ,\n",
       "       0.00500501, 0.00600601, 0.00700701, 0.00800801, 0.00900901,\n",
       "       0.01001001, 0.01101101, 0.01201201, 0.01301301, 0.01401401,\n",
       "       0.01501502, 0.01601602, 0.01701702, 0.01801802, 0.01901902,\n",
       "       0.02002002, 0.02102102, 0.02202202, 0.02302302, 0.02402402,\n",
       "       0.02502503, 0.02602603, 0.02702703, 0.02802803, 0.02902903,\n",
       "       0.03003003, 0.0...\n",
       "       0.97597598, 0.97697698, 0.97797798, 0.97897898, 0.97997998,\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "        cv=10, fit_intercept=True, gcv_mode=None, normalize=True, scoring=None,\n",
       "        store_cv_values=False)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7627627627627628"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_tuned = Ridge(alpha = ridge_cv.alpha_, \n",
    "                   normalize = True).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Model RMSE - Alpha3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471.71950807754035"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, ridge_tuned.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning - AlphaDefault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "ridge_cv = RidgeCV( \n",
    "                   cv=10,\n",
    "                   normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([ 0.1,  1. , 10. ]), cv=10, fit_intercept=True,\n",
       "        gcv_mode=None, normalize=True, scoring=None, store_cv_values=False)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_tuned = Ridge(alpha = ridge_cv.alpha_, \n",
    "                   normalize = True).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Model RMSE - AlphaDefault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "470.4092515467964"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, ridge_tuned.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import RidgeCV, LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "      <th>League_N</th>\n",
       "      <th>Division_W</th>\n",
       "      <th>NewLeague_N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>446</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>69</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>475.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>63</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>480.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>225</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>91.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n",
       "0    293    66      1    30   29     14      1     293     66       1     30   \n",
       "1    315    81      7    24   38     39     14    3449    835      69    321   \n",
       "2    479   130     18    66   72     76      3    1624    457      63    224   \n",
       "3    496   141     20    65   78     37     11    5628   1575     225    828   \n",
       "4    321    87     10    39   42     30      2     396    101      12     48   \n",
       "\n",
       "   CRBI  CWalks  PutOuts  Assists  Errors  Salary  League_N  Division_W  \\\n",
       "0    29      14      446       33      20     NaN         0           0   \n",
       "1   414     375      632       43      10   475.0         1           1   \n",
       "2   266     263      880       82      14   480.0         0           1   \n",
       "3   838     354      200       11       3   500.0         1           0   \n",
       "4    46      33      805       40       4    91.5         1           0   \n",
       "\n",
       "   NewLeague_N  \n",
       "0            0  \n",
       "1            1  \n",
       "2            0  \n",
       "3            1  \n",
       "4            1  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df1[\"Salary\"]\n",
    "X = df1.drop('Salary', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7225357.87715617, tolerance: 3956.6756074595623\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "lasso_model = Lasso().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.920879494234896"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.31749777,  5.57823278,  4.1259529 ,  0.6869569 , -0.67490008,\n",
       "        2.14209473, -0.99924504, -0.37677018,  1.60363828,  2.32088645,\n",
       "        0.42846628, -0.74637135, -0.04325485,  0.33268874])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Egitim Seti Hatasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263.8709208800576"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lasso_model.predict(X_train)\n",
    "np.sqrt(mean_squared_error(y_train,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Seti Hatasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493.173554073917"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lasso_model.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning - Alpha1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas1 = np.random.randint(0,1100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "lasso_cv = LassoCV(alphas = alphas1, \n",
    "                   cv=10,\n",
    "                   normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=array([569]), copy_X=True, cv=10, eps=0.001, fit_intercept=True,\n",
       "        max_iter=1000, n_alphas=100, n_jobs=None, normalize=True,\n",
       "        positive=False, precompute='auto', random_state=None,\n",
       "        selection='cyclic', tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_tuned = Lasso(alpha = lasso_cv.alpha_, \n",
    "                   normalize = True).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Model RMSE - Alpha1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509.5000845689271"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, lasso_tuned.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning - Alpha2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas2 = 10**np.linspace(10,-2,100)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "lasso_cv = LassoCV(alphas = alphas2, \n",
    "                   cv=10,\n",
    "                   normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3681.1861916929483, tolerance: 3547.539874726709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14091.38972793147, tolerance: 3547.539874726709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16292.497580293566, tolerance: 3547.539874726709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15343.006629239768, tolerance: 3547.539874726709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14409.122455166653, tolerance: 3547.539874726709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13654.30327332206, tolerance: 3547.539874726709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12272.071822922677, tolerance: 3547.539874726709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18761.241059094667, tolerance: 3547.539874726709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4125.519517444074, tolerance: 3547.539874726709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5762.993547406048, tolerance: 3547.539874726709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5914.224065959454, tolerance: 3297.418393205709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15450.688587768003, tolerance: 3297.418393205709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15357.136785537004, tolerance: 3297.418393205709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14053.118978476152, tolerance: 3297.418393205709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12855.513904929161, tolerance: 3297.418393205709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6943.782145693898, tolerance: 3297.418393205709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8100.463492549956, tolerance: 3297.418393205709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13246.33517287299, tolerance: 3297.418393205709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13443.156311254948, tolerance: 3297.418393205709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13170.065466744825, tolerance: 3297.418393205709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4049.8523151278496, tolerance: 3735.405315825182\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9666.57278503105, tolerance: 3735.405315825182\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13266.85607143864, tolerance: 3735.405315825182\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10932.423553165048, tolerance: 3735.405315825182\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9463.10482788831, tolerance: 3735.405315825182\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9005.320162184536, tolerance: 3735.405315825182\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16889.865436885506, tolerance: 3735.405315825182\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 17534.98869566992, tolerance: 3735.405315825182\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18488.674973078072, tolerance: 3735.405315825182\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5106.829689662904, tolerance: 3395.622404008858\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5664.562415905297, tolerance: 3395.622404008858\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5336.242226108909, tolerance: 3395.622404008858\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5014.084202805534, tolerance: 3395.622404008858\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4760.020462028682, tolerance: 3395.622404008858\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4564.646460449323, tolerance: 3395.622404008858\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3839.306803610176, tolerance: 3651.8483906293914\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10441.737436972558, tolerance: 3651.8483906293914\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 22208.626360638067, tolerance: 3651.8483906293914\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5606.055958833545, tolerance: 3651.8483906293914\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10741.653568714857, tolerance: 3651.8483906293914\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26415.79342920333, tolerance: 3651.8483906293914\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 28878.728668875992, tolerance: 3651.8483906293914\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 28779.55967802182, tolerance: 3651.8483906293914\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 28366.506013516337, tolerance: 3651.8483906293914\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9299.719867909327, tolerance: 3342.470864504487\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11395.35815326497, tolerance: 3342.470864504487\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10536.27012971975, tolerance: 3342.470864504487\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9593.010908052325, tolerance: 3342.470864504487\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11239.942226104438, tolerance: 3342.470864504487\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20485.461861591786, tolerance: 3342.470864504487\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6549.605666425079, tolerance: 3342.470864504487\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12477.657875861973, tolerance: 3681.6436895585152\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12564.78728653863, tolerance: 3681.6436895585152\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11664.169651566073, tolerance: 3681.6436895585152\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10824.688479389995, tolerance: 3681.6436895585152\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10138.290817596018, tolerance: 3681.6436895585152\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9593.466531224549, tolerance: 3681.6436895585152\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9167.053900782019, tolerance: 3681.6436895585152\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8836.357168264687, tolerance: 3681.6436895585152\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8581.547877868637, tolerance: 3681.6436895585152\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4433.590468624607, tolerance: 3684.426158113166\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3880.486788097769, tolerance: 3684.426158113166\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10889.272952005267, tolerance: 3684.426158113166\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15534.317116826773, tolerance: 3684.426158113166\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15720.675490844995, tolerance: 3684.426158113166\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15430.0691607818, tolerance: 3684.426158113166\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15155.389865960926, tolerance: 3684.426158113166\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7132.907690115273, tolerance: 3524.5040211452874\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10557.818431463093, tolerance: 3524.5040211452874\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10165.59592582658, tolerance: 3524.5040211452874\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9474.316246189177, tolerance: 3524.5040211452874\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8885.191748596728, tolerance: 3524.5040211452874\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8418.862290881574, tolerance: 3524.5040211452874\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8056.111433561891, tolerance: 3524.5040211452874\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7776.186022449285, tolerance: 3524.5040211452874\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7561.296873617917, tolerance: 3524.5040211452874\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9320.232206143439, tolerance: 3724.140602819258\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8295.778306577355, tolerance: 3724.140602819258\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13276.30704946071, tolerance: 3724.140602819258\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12581.516210684553, tolerance: 3724.140602819258\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10762.61804201454, tolerance: 3724.140602819258\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9430.157088253647, tolerance: 3724.140602819258\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9165.617179274559, tolerance: 3724.140602819258\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9634.475455388427, tolerance: 3724.140602819258\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9269.910740923136, tolerance: 3724.140602819258\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8916.330367349088, tolerance: 3724.140602819258\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8637.65499895811, tolerance: 3724.140602819258\n",
      "  tol, rng, random, positive)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=array([5.00000000e+09, 3.78231664e+09, 2.86118383e+09, 2.16438064e+09,\n",
       "       1.63727458e+09, 1.23853818e+09, 9.36908711e+08, 7.08737081e+08,\n",
       "       5.36133611e+08, 4.05565415e+08, 3.06795364e+08, 2.32079442e+08,\n",
       "       1.75559587e+08, 1.32804389e+08, 1.00461650e+08, 7.59955541e+07,\n",
       "       5.74878498e+07, 4.34874501e+07, 3.28966612e+07, 2.48851178e+07,\n",
       "       1.88246790e+07, 1.42401793e+0...\n",
       "       1.07721735e-01, 8.14875417e-02, 6.16423370e-02, 4.66301673e-02,\n",
       "       3.52740116e-02, 2.66834962e-02, 2.01850863e-02, 1.52692775e-02,\n",
       "       1.15506485e-02, 8.73764200e-03, 6.60970574e-03, 5.00000000e-03]),\n",
       "        copy_X=True, cv=10, eps=0.001, fit_intercept=True, max_iter=1000,\n",
       "        n_alphas=100, n_jobs=None, normalize=True, positive=False,\n",
       "        precompute='auto', random_state=None, selection='cyclic', tol=0.0001,\n",
       "        verbose=False)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08148754173103201"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_tuned = Lasso(alpha = lasso_cv.alpha_, \n",
    "                   normalize = True).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Model RMSE - Alpha2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "473.60005163449443"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, lasso_tuned.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning - Alpha3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas3 = np.linspace(0,1,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "lasso_cv = LassoCV(alphas = alphas3, \n",
    "                   cv=10,\n",
    "                   normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3683.8918730206788, tolerance: 3547.539874726709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14571.864924788475, tolerance: 3547.539874726709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26809.9890762344, tolerance: 3547.539874726709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56441.98680303246, tolerance: 3547.539874726709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 188421.7131772302, tolerance: 3547.539874726709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6884163.042096905, tolerance: 3547.539874726709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3900.823112329468, tolerance: 3297.418393205709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5250.639327611774, tolerance: 3297.418393205709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7730.286848179996, tolerance: 3297.418393205709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12947.101811561733, tolerance: 3297.418393205709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27115.25199180469, tolerance: 3297.418393205709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 93987.30733713508, tolerance: 3297.418393205709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6301502.85859479, tolerance: 3297.418393205709\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3883.6938936356455, tolerance: 3735.405315825182\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4919.097869820893, tolerance: 3735.405315825182\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7376.313730306923, tolerance: 3735.405315825182\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11517.832606904209, tolerance: 3735.405315825182\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 19535.48053208366, tolerance: 3735.405315825182\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 40966.019118037075, tolerance: 3735.405315825182\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 139661.92801907286, tolerance: 3735.405315825182\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6845827.834081084, tolerance: 3735.405315825182\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4485.552897647023, tolerance: 3395.622404008858\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9221.305093334988, tolerance: 3395.622404008858\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 28296.489041425288, tolerance: 3395.622404008858\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6008722.909868881, tolerance: 3395.622404008858\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3978.867761641741, tolerance: 3651.8483906293914\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4764.5145997442305, tolerance: 3651.8483906293914\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5946.828948948532, tolerance: 3651.8483906293914\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 24842.891135690734, tolerance: 3651.8483906293914\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 116008.24793079868, tolerance: 3651.8483906293914\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 382099.6452843603, tolerance: 3651.8483906293914\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6235757.525232777, tolerance: 3651.8483906293914\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3876.658743392676, tolerance: 3342.470864504487\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4788.206926181912, tolerance: 3342.470864504487\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6184.361100032926, tolerance: 3342.470864504487\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6911.507585640997, tolerance: 3342.470864504487\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 24791.37700118497, tolerance: 3342.470864504487\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 53667.664445891976, tolerance: 3342.470864504487\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 178648.8591172807, tolerance: 3342.470864504487\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6380672.522978574, tolerance: 3342.470864504487\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5039.71393618919, tolerance: 3681.6436895585152\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8381.736049128696, tolerance: 3681.6436895585152\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 17624.94990207255, tolerance: 3681.6436895585152\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 62523.88814451359, tolerance: 3681.6436895585152\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6621364.92580945, tolerance: 3681.6436895585152\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4350.358214326203, tolerance: 3684.426158113166\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5915.934520389885, tolerance: 3684.426158113166\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8805.161143548787, tolerance: 3684.426158113166\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14880.604651108384, tolerance: 3684.426158113166\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 31349.07514936477, tolerance: 3684.426158113166\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 108689.81777131557, tolerance: 3684.426158113166\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6811467.886126071, tolerance: 3684.426158113166\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4448.802243873477, tolerance: 3524.5040211452874\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7361.1333045475185, tolerance: 3524.5040211452874\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15528.546467751265, tolerance: 3524.5040211452874\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 55509.54838100448, tolerance: 3524.5040211452874\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6730068.840596896, tolerance: 3524.5040211452874\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5081.4475567266345, tolerance: 3724.140602819258\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8414.081808833405, tolerance: 3724.140602819258\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 17615.32777627185, tolerance: 3724.140602819258\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 62113.031321730465, tolerance: 3724.140602819258\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:472: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6067838.2958885655, tolerance: 3724.140602819258\n",
      "  tol, rng, random, positive)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=array([0.        , 0.001001  , 0.002002  , 0.003003  , 0.004004  ,\n",
       "       0.00500501, 0.00600601, 0.00700701, 0.00800801, 0.00900901,\n",
       "       0.01001001, 0.01101101, 0.01201201, 0.01301301, 0.01401401,\n",
       "       0.01501502, 0.01601602, 0.01701702, 0.01801802, 0.01901902,\n",
       "       0.02002002, 0.02102102, 0.02202202, 0.02302302, 0.02402402,\n",
       "       0.02502503, 0.02602603, 0.02702703, 0.02802803, 0.02902903,\n",
       "       0.03003003, 0.0...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "        copy_X=True, cv=10, eps=0.001, fit_intercept=True, max_iter=1000,\n",
       "        n_alphas=100, n_jobs=None, normalize=True, positive=False,\n",
       "        precompute='auto', random_state=None, selection='cyclic', tol=0.0001,\n",
       "        verbose=False)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09109109109109109"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_tuned = Lasso(alpha = lasso_cv.alpha_, \n",
    "                   normalize = True).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Model RMSE - Alpha3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472.9025899415753"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, lasso_tuned.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning - AlphaDefault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "lasso_cv = LassoCV( \n",
    "                   cv=10,\n",
    "                   max_iter = 10000,\n",
    "                   normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=None, copy_X=True, cv=10, eps=0.001, fit_intercept=True,\n",
       "        max_iter=10000, n_alphas=100, n_jobs=None, normalize=True,\n",
       "        positive=False, precompute='auto', random_state=None,\n",
       "        selection='cyclic', tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0912302497100227"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_tuned = Lasso(alpha = lasso_cv.alpha_, \n",
    "                   normalize = True).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Model RMSE - AlphaDefault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472.89284127811334"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, lasso_tuned.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElasticNet Reg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import RidgeCV, LassoCV,ElasticNetCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df1[\"Salary\"]\n",
    "X = df1.drop('Salary', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ts-omer.ari\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7270439.220481439, tolerance: 3956.6756074595623\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "elasticnet_model = ElasticNet().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.83810752001932"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elasticnet_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.31033131,  5.52315901,  3.97245359,  0.74179022, -0.62210674,\n",
       "        2.11868526, -0.98119602, -0.37808488,  1.61269849,  2.33965529,\n",
       "        0.42114582, -0.75391067, -0.04025577,  0.3327871 ])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elasticnet_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Egitim Seti Hatasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263.8706761421944"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = elasticnet_model.predict(X_train)\n",
    "np.sqrt(mean_squared_error(y_train,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Seti Hatasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493.2449506799692"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = elasticnet_model.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas1 = np.random.randint(0,1100,1)\n",
    "alphas2 = 10**np.linspace(10,-2,100)*0.5\n",
    "alphas3 = np.linspace(0,1,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mElasticNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0ml1_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mprecompute\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcopy_X\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mselection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cyclic'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Linear regression with combined L1 and L2 priors as regularizer.\n",
       "\n",
       "Minimizes the objective function::\n",
       "\n",
       "        1 / (2 * n_samples) * ||y - Xw||^2_2\n",
       "        + alpha * l1_ratio * ||w||_1\n",
       "        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
       "\n",
       "If you are interested in controlling the L1 and L2 penalty\n",
       "separately, keep in mind that this is equivalent to::\n",
       "\n",
       "        a * L1 + b * L2\n",
       "\n",
       "where::\n",
       "\n",
       "        alpha = a + b and l1_ratio = a / (a + b)\n",
       "\n",
       "The parameter l1_ratio corresponds to alpha in the glmnet R package while\n",
       "alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n",
       "= 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\n",
       "unless you supply your own sequence of alpha.\n",
       "\n",
       "Read more in the :ref:`User Guide <elastic_net>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "alpha : float, optional\n",
       "    Constant that multiplies the penalty terms. Defaults to 1.0.\n",
       "    See the notes for the exact mathematical meaning of this\n",
       "    parameter. ``alpha = 0`` is equivalent to an ordinary least square,\n",
       "    solved by the :class:`LinearRegression` object. For numerical\n",
       "    reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
       "    Given this, you should use the :class:`LinearRegression` object.\n",
       "\n",
       "l1_ratio : float\n",
       "    The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n",
       "    ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n",
       "    is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n",
       "    combination of L1 and L2.\n",
       "\n",
       "fit_intercept : bool\n",
       "    Whether the intercept should be estimated or not. If ``False``, the\n",
       "    data is assumed to be already centered.\n",
       "\n",
       "normalize : boolean, optional, default False\n",
       "    This parameter is ignored when ``fit_intercept`` is set to False.\n",
       "    If True, the regressors X will be normalized before regression by\n",
       "    subtracting the mean and dividing by the l2-norm.\n",
       "    If you wish to standardize, please use\n",
       "    :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
       "    on an estimator with ``normalize=False``.\n",
       "\n",
       "precompute : True | False | array-like\n",
       "    Whether to use a precomputed Gram matrix to speed up\n",
       "    calculations. The Gram matrix can also be passed as argument.\n",
       "    For sparse input this option is always ``True`` to preserve sparsity.\n",
       "\n",
       "max_iter : int, optional\n",
       "    The maximum number of iterations\n",
       "\n",
       "copy_X : boolean, optional, default True\n",
       "    If ``True``, X will be copied; else, it may be overwritten.\n",
       "\n",
       "tol : float, optional\n",
       "    The tolerance for the optimization: if the updates are\n",
       "    smaller than ``tol``, the optimization code checks the\n",
       "    dual gap for optimality and continues until it is smaller\n",
       "    than ``tol``.\n",
       "\n",
       "warm_start : bool, optional\n",
       "    When set to ``True``, reuse the solution of the previous call to fit as\n",
       "    initialization, otherwise, just erase the previous solution.\n",
       "    See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "positive : bool, optional\n",
       "    When set to ``True``, forces the coefficients to be positive.\n",
       "\n",
       "random_state : int, RandomState instance or None, optional, default None\n",
       "    The seed of the pseudo random number generator that selects a random\n",
       "    feature to update.  If int, random_state is the seed used by the random\n",
       "    number generator; If RandomState instance, random_state is the random\n",
       "    number generator; If None, the random number generator is the\n",
       "    RandomState instance used by `np.random`. Used when ``selection`` ==\n",
       "    'random'.\n",
       "\n",
       "selection : str, default 'cyclic'\n",
       "    If set to 'random', a random coefficient is updated every iteration\n",
       "    rather than looping over features sequentially by default. This\n",
       "    (setting to 'random') often leads to significantly faster convergence\n",
       "    especially when tol is higher than 1e-4.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "coef_ : array, shape (n_features,) | (n_targets, n_features)\n",
       "    parameter vector (w in the cost function formula)\n",
       "\n",
       "sparse_coef_ : scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features)\n",
       "    ``sparse_coef_`` is a readonly property derived from ``coef_``\n",
       "\n",
       "intercept_ : float | array, shape (n_targets,)\n",
       "    independent term in decision function.\n",
       "\n",
       "n_iter_ : array-like, shape (n_targets,)\n",
       "    number of iterations run by the coordinate descent solver to reach\n",
       "    the specified tolerance.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.linear_model import ElasticNet\n",
       ">>> from sklearn.datasets import make_regression\n",
       "\n",
       ">>> X, y = make_regression(n_features=2, random_state=0)\n",
       ">>> regr = ElasticNet(random_state=0)\n",
       ">>> regr.fit(X, y)\n",
       "ElasticNet(random_state=0)\n",
       ">>> print(regr.coef_)\n",
       "[18.83816048 64.55968825]\n",
       ">>> print(regr.intercept_)\n",
       "1.451...\n",
       ">>> print(regr.predict([[0, 0]]))\n",
       "[1.451...]\n",
       "\n",
       "\n",
       "Notes\n",
       "-----\n",
       "To avoid unnecessary memory duplication the X argument of the fit method\n",
       "should be directly passed as a Fortran-contiguous numpy array.\n",
       "\n",
       "See also\n",
       "--------\n",
       "ElasticNetCV : Elastic net model with best model selection by\n",
       "    cross-validation.\n",
       "SGDRegressor: implements elastic net regression with incremental training.\n",
       "SGDClassifier: implements logistic regression with elastic net penalty\n",
       "    (``SGDClassifier(loss=\"log\", penalty=\"elasticnet\")``).\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\ts-omer.ari\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     Lasso\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_cv_model = ElasticNetCV( cv = 10, random_state = 46).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1081.386118463718"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet_cv_model.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Model RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "485.0895200560485"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet_tuned = ElasticNet(alpha = enet_cv_model.alpha_).fit(X_train, y_train)\n",
    "y_pred = enet_tuned.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe standartizasyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X.columns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
